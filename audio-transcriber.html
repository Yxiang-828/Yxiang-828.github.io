<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Transcriber - Yao Xiang</title>
    <link rel="stylesheet" href="styles.css">
    <style>
        .detail-container { max-width: 1200px; margin: 100px auto 50px; padding: 50px; background: rgba(15, 15, 35, 0.9); border-radius: 20px; border: 1px solid rgba(255,255,255,0.1); }
        .back-link { display: inline-block; margin-bottom: 30px; color: #00d4ff; text-decoration: none; font-weight: bold; transition: all 0.3s; }
        .back-link:hover { transform: translateX(-5px); }
        .tech-stack { display: flex; flex-wrap: wrap; gap: 10px; margin: 20px 0; }
        .tech-badge { background: linear-gradient(45deg, #00d4ff, #090979); color: white; padding: 8px 16px; border-radius: 20px; font-size: 0.9rem; font-weight: bold; }
        .perf-table { width: 100%; margin: 30px 0; border-collapse: collapse; }
        .perf-table th, .perf-table td { padding: 15px; text-align: left; border-bottom: 1px solid rgba(255,255,255,0.1); }
        .perf-table th { background: rgba(0, 212, 255, 0.1); color: #00d4ff; }
        .alert-box { background: rgba(255, 107, 107, 0.1); border-left: 4px solid #ff6b6b; padding: 20px; margin: 20px 0; border-radius: 8px; }
        .feature-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 30px 0; }
        .feature-box { background: rgba(0, 212, 255, 0.05); padding: 20px; border-radius: 10px; border-left: 3px solid #00d4ff; }
        code { background: rgba(0,0,0,0.4); padding: 3px 8px; border-radius: 4px; color: #ff6b6b; font-family: 'Courier New', monospace; }
        pre { background: rgba(0,0,0,0.6); padding: 20px; border-radius: 10px; overflow-x: auto; border-left: 3px solid #00d4ff; }
        pre code { background: none; padding: 0; color: #e0e0e0; }
        .section-title { color: #00d4ff; margin-top: 50px; padding-bottom: 10px; border-bottom: 2px solid rgba(0, 212, 255, 0.3); }
    </style>
</head>
<body>
    <header>
        <nav>
            <div class="nav-container">
                <h1><a href="index.html" style="color: inherit; text-decoration: none;">Yao Xiang</a></h1>
                <ul>
                    <li><a href="index.html#about">About</a></li>
                    <li><a href="index.html#projects">Projects</a></li>
                    <li><a href="index.html#skills">Skills</a></li>
                    <li><a href="index.html#contact">Contact</a></li>
                </ul>
            </div>
        </nav>
    </header>

    <div class="detail-container">
        <a href="index.html#projects" class="back-link">‚Üê Back to Projects</a>
        
        <h1 style="color: #00d4ff; margin-bottom: 20px;">üé§ Audio to Text Transcriber</h1>
        
        <div class="tech-stack">
            <span class="tech-badge">Whisper AI</span>
            <span class="tech-badge">ONNX Runtime</span>
            <span class="tech-badge">DirectML</span>
            <span class="tech-badge">Python 3.13</span>
            <span class="tech-badge">AMD GPU</span>
            <span class="tech-badge">FFmpeg</span>
        </div>

        <p style="font-size: 1.2rem; color: #b0b0b0; margin: 30px 0;">
            I built a custom Python pipeline that leverages Whisper AI with AMD GPU acceleration via DirectML. The architecture is purposefully restrictive - targeting only AMD Radeon RX 6000/7000/8000 series GPUs on Windows to maximize DirectML optimization. My implementation handles long-form audio (30+ minutes) through intelligent chunking and achieves 3-5x speedup over CPU processing.
        </p>

        <h2 class="section-title">üîß My Implementation</h2>
        <div class="feature-grid">
            <div class="feature-box">
                <h3 style="color: #00d4ff;">Custom Chunking Pipeline</h3>
                <p style="color: #b0b0b0;">I engineered a 30-second audio segmentation system that overcomes Whisper's token limits, enabling full transcription of 20+ minute files</p>
            </div>
            <div class="feature-box">
                <h3 style="color: #00d4ff;">GPU Memory Management</h3>
                <p style="color: #b0b0b0;">I wrote explicit tensor-to-device transfers (<code>input_features.to(device)</code>) to force GPU utilization, reducing CPU load from 70% to 20-30%</p>
            </div>
            <div class="feature-box">
                <h3 style="color: #00d4ff;">ONNX Conversion Layer</h3>
                <p style="color: #b0b0b0;">My code automatically converts Whisper models to ONNX format on first run, then caches them for 5-10x faster subsequent executions</p>
            </div>
            <div class="feature-box">
                <h3 style="color: #00d4ff;">Real-Time Progress System</h3>
                <p style="color: #b0b0b0;">I built a custom progress bar with per-chunk timing analytics, displaying real-time factor and GPU confirmation</p>
            </div>
            <div class="feature-box">
                <h3 style="color: #00d4ff;">Automated Setup Script</h3>
                <p style="color: #b0b0b0;">I created <code>setup.bat</code> that validates Python version, installs DirectML dependencies, and verifies GPU acceleration in one command</p>
            </div>
            <div class="feature-box">
                <h3 style="color: #00d4ff;">Error Handling & Fallbacks</h3>
                <p style="color: #b0b0b0;">My pipeline gracefully handles missing GPUs, corrupted audio, and model download failures with detailed error messages</p>
            </div>
        </div>

        <div class="alert-box">
            <h3 style="color: #ff6b6b; margin-top: 0;">‚ö†Ô∏è Design Decision: Hardware Restrictions</h3>
            <p style="color: #b0b0b0; margin-bottom: 0;">
                I intentionally limited this tool to:<br>
                ‚Ä¢ AMD Radeon RX 6000/7000/8000 series GPUs only<br>
                ‚Ä¢ Windows 10/11 (native, no WSL/Linux/macOS)<br>
                ‚Ä¢ Python 3.13.x specifically<br><br>
                This restriction allows me to optimize exclusively for DirectML without maintaining CUDA/ROCm branches, resulting in cleaner code and better AMD performance.
            </p>
        </div>

        <h2 class="section-title">üìä Performance Benchmarks (AMD RX 6800 XT)</h2>
        <table class="perf-table">
            <thead>
                <tr>
                    <th>Model</th>
                    <th>First Run</th>
                    <th>Cached Runs</th>
                    <th>VRAM Usage</th>
                    <th>Use Case</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>tiny</strong></td>
                    <td>5-10 sec</td>
                    <td><strong>2-3 sec</strong></td>
                    <td>~1GB</td>
                    <td>Testing/fast transcription</td>
                </tr>
                <tr>
                    <td><strong>base</strong></td>
                    <td>10-20 sec</td>
                    <td><strong>3-5 sec</strong></td>
                    <td>~2GB</td>
                    <td><span style="color: #00d4ff;">Recommended</span></td>
                </tr>
                <tr>
                    <td><strong>small</strong></td>
                    <td>30-60 sec</td>
                    <td>8-12 sec</td>
                    <td>~4GB</td>
                    <td>Better accuracy</td>
                </tr>
                <tr>
                    <td><strong>medium</strong></td>
                    <td>2-3 min</td>
                    <td>15-25 sec</td>
                    <td>~8GB</td>
                    <td>High accuracy</td>
                </tr>
                <tr>
                    <td><strong>large</strong></td>
                    <td>3-5 min</td>
                    <td>30-60 sec</td>
                    <td>~16GB</td>
                    <td>Best accuracy</td>
                </tr>
            </tbody>
        </table>
        <p style="color: #b0b0b0;"><em>Note: Times are for 30 seconds of audio. First run includes ONNX conversion (one-time cost). Models cache automatically after first use.</em></p>

        <h2 class="section-title">üöÄ Setup (One-Command)</h2>
        <pre><code>cd "C:\Program Files (x86)\helper_tools\Audio_to_Text_Transcriber"
setup.bat</code></pre>
        
        <h3 style="color: #ff6b6b; margin-top: 30px;">What setup.bat does:</h3>
        <ol style="color: #b0b0b0; line-height: 2;">
            <li>Verifies Python 3.13.x installation</li>
            <li>Installs PyTorch (CPU version)</li>
            <li>Installs ONNX Runtime DirectML (AMD GPU acceleration)</li>
            <li>Installs Optimum (ONNX optimization)</li>
            <li>Installs Librosa (audio processing)</li>
            <li>Verifies DirectML GPU acceleration</li>
            <li>Optional: Pre-downloads Whisper models</li>
        </ol>

        <h2 class="section-title">üíª Usage</h2>
        <h3 style="color: #ff6b6b;">Interactive Mode (Recommended)</h3>
        <pre><code># From helper_tools root
mp3-to-txt.bat "path/to/audio.mp3"</code></pre>
        
        <h3 style="color: #ff6b6b; margin-top: 20px;">Command Line</h3>
        <pre><code>py audio_to_text.py "audio.mp3" --model base --language en</code></pre>

        <h3 style="color: #ff6b6b; margin-top: 20px;">Example Output</h3>
        <pre><code>======================================================================
‚úÖ TRANSCRIPTION COMPLETE
======================================================================
Total time: 85.42s
Audio duration: 1242.8s (20.7 minutes)
Average per chunk: 2.03s
Real-time factor: 0.07x (lower is faster)
Total characters: 18547
======================================================================</code></pre>

        <h2 class="section-title">üîß Engineering Evolution: v2.0 Rewrite</h2>
        
        <h3 style="color: #ff6b6b;">Problem I Solved: Token Limit Bottleneck</h3>
        <p style="color: #b0b0b0;">Whisper's original implementation only processed the first 30 seconds of audio due to token limits. I re-architected the entire pipeline to handle unlimited audio length:</p>
        <ul style="color: #b0b0b0; line-height: 2;">
            <li><strong>My Solution:</strong> Built a chunking system using Librosa to split audio into 30-second segments</li>
            <li><strong>Implementation:</strong> Process each chunk independently on GPU, then concatenate transcriptions</li>
            <li><strong>Result:</strong> Now handles 20+ minute files that previously failed</li>
        </ul>

        <h3 style="color: #ff6b6b; margin-top: 20px;">My Custom Progress Bar System</h3>
        <pre><code>[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 50.0% - Chunk 21/42 (30.0s audio)
Processing time: 2.03s | GPU: DirectML (AMD Radeon RX 6800 XT)</code></pre>
        <p style="color: #b0b0b0;">I wrote a real-time progress visualization that displays:</p>
        <ul style="color: #b0b0b0; line-height: 2;">
            <li><strong>Visual Bar:</strong> Custom ASCII rendering with percentage calculation</li>
            <li><strong>Chunk Counter:</strong> Current/total segments with timing per chunk</li>
            <li><strong>GPU Confirmation:</strong> Device detection to verify DirectML is active</li>
            <li><strong>Performance Metrics:</strong> Real-time factor calculation for speed analysis</li>
        </ul>

        <h3 style="color: #ff6b6b; margin-top: 20px;">GPU Optimization I Implemented</h3>
        <p style="color: #b0b0b0;">I discovered that Whisper's default behavior processes features on CPU even with GPU available. My fix:</p>
        <pre><code># My explicit GPU transfer code
input_features = processor(audio, return_tensors="pt").input_features
input_features = input_features.to(device)  # Force DirectML device
predicted_ids = model.generate(input_features)  # Now runs on GPU</code></pre>
        <ul style="color: #b0b0b0; line-height: 2;">
            <li><strong>Before My Fix:</strong> CPU stuck at 70% usage, GPU idle</li>
            <li><strong>After My Fix:</strong> CPU drops to 20-30%, GPU utilization 70-80%</li>
            <li><strong>Performance Gain:</strong> ~15x real-time factor (2 seconds per 30-second chunk)</li>
        </ul>

        <h2 class="section-title">üìÅ Supported Formats</h2>
        <ul style="color: #b0b0b0; line-height: 2;">
            <li><strong>Audio:</strong> mp3, wav, m4a, flac, ogg, aac</li>
            <li><strong>Video:</strong> mp4, avi, mov, mkv (audio extracted automatically)</li>
        </ul>

        <h2 class="section-title">üîß Troubleshooting</h2>
        
        <h3 style="color: #ff6b6b;">"DmlExecutionProvider not available"</h3>
        <pre><code>py -m pip uninstall onnxruntime-directml -y
py -m pip install onnxruntime-directml --force-reinstall</code></pre>

        <h3 style="color: #ff6b6b; margin-top: 20px;">AMD GPU not detected</h3>
        <ul style="color: #b0b0b0; line-height: 2;">
            <li>Update AMD drivers via AMD Adrenalin software</li>
            <li>Ensure GPU is not in power-saving mode</li>
            <li>Check Windows Device Manager for GPU status</li>
        </ul>

        <div style="text-align: center; margin-top: 50px;">
            <a href="https://github.com/Yxiang-828/Helper_Tools/tree/main/Audio_to_Text_Transcriber" target="_blank" class="cta-button">View on GitHub ‚Üí</a>
        </div>
    </div>

    <footer><div class="container"><p>&copy; 2025 Yao Xiang. All rights reserved.</p></div></footer>
</body>
</html>
